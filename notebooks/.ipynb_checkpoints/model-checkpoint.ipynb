{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import h5py as h5\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12602\n",
    "MAX_QUESTION_LEN = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def qa_loss(y_true, y_pred):\n",
    "#     print(y_true[0])\n",
    "#     print(y_pred[0])\n",
    "#     q_true, a_true = y_true[0], y_true[1]\n",
    "#     q_pred, a_pred = y_pred[0][:-1], y_pred[1]\n",
    "    \n",
    "#     q_loss = K.sum(q_true * K.log(q_pred), axis=-1)\n",
    "#     q_loss = K.sum(q_loss, axis=-1)\n",
    "#     a_loss = K.sum(a_true * K.log(a_pred), axis=-1)\n",
    "#     loss = K.sum(y_true * K.log(y_pred), axis=-1)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQANet:\n",
    "    def __init__(self, combine_type, question_embed_dim, lstm_dim, n_answers):\n",
    "        self.combine_type = combine_type\n",
    "        self.question_embed_dim = question_embed_dim\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.n_answers = n_answers\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        if self.combine_type == 'show-and-tell':\n",
    "            image_features = tf.keras.layers.Input(shape=(4096,), \n",
    "                                                   dtype='float32')\n",
    "            \n",
    "            image_embedding = tf.keras.layers.Dense(units=self.question_embed_dim, \n",
    "                                                    activation='elu',\n",
    "                                                    name='image_embedding')(inputs=image_features)\n",
    "\n",
    "            image_embedding = tf.keras.layers.Reshape((1, self.question_embed_dim))(image_embedding)\n",
    "            \n",
    "            question_input = tf.keras.layers.Input(shape=(MAX_QUESTION_LEN,), \n",
    "                                                   dtype='int32',\n",
    "                                                   name='question_input')\n",
    "            \n",
    "            question_embedding = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
    "                                                           output_dim=self.question_embed_dim, \n",
    "                                                           input_length=MAX_QUESTION_LEN,\n",
    "                                                           name='question_embedding')(inputs=question_input)\n",
    "            \n",
    "            image_question_embedding = tf.keras.layers.Concatenate(axis=1, \n",
    "                                                                   name='image_question_embedding')(inputs=[image_embedding, question_embedding])\n",
    "            \n",
    "            question_features, last_h, _ = tf.keras.layers.LSTM(units=self.lstm_dim, \n",
    "                                                                return_sequences=True, \n",
    "                                                                return_state=True, \n",
    "                                                                name='question_generator')(inputs=image_question_embedding)\n",
    "\n",
    "            question_pred = tf.keras.layers.TimeDistributed(layer=tf.keras.layers.Dense(units=VOCAB_SIZE, \n",
    "                                                                  activation='softmax', \n",
    "                                                                  name='word_classifier'))(inputs=question_features)\n",
    "            \n",
    "            # question_pred[:-1] ignores the last output. Need to add <START> and <END>.\n",
    "            question_pred = tf.keras.layers.Lambda(lambda x: x[:, :-1, :], \n",
    "                                                   name='ignore_last_word')(inputs=question_pred)\n",
    "            \n",
    "            answer_fc1 = tf.keras.layers.Dense(units=1000,\n",
    "                                                activation='elu',\n",
    "                                                name='answer_dense_1')(inputs=last_h)\n",
    "            \n",
    "            answer_fc2 = tf.keras.layers.Dense(units=1000,\n",
    "                                                activation='elu',\n",
    "                                                name='answer_dense_2')(inputs=answer_fc1)\n",
    "            \n",
    "            answer_pred = tf.keras.layers.Dense(units=self.n_answers,\n",
    "                                                activation='softmax',\n",
    "                                                name='answer_classifier')(inputs=answer_features)\n",
    "            \n",
    "            self.model = tf.keras.Model(inputs=[image_features, question_input], \n",
    "                                        outputs=[question_pred, answer_pred])  \n",
    "            \n",
    "            self.model.compile(loss='categorical_crossentropy', \n",
    "                               optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val, batch_size, epochs):\n",
    "        self.model.fit(x=x_train, \n",
    "                       y=y_train, \n",
    "                       batch_size=batch_size, \n",
    "                       epochs=epochs, \n",
    "                       verbose=1,\n",
    "                       validation_split=0.2,\n",
    "                       shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "question_embed_dim = 256\n",
    "lstm_dim = 512\n",
    "n_answers = 1000\n",
    "n_train = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_embedding (Dense)         (None, 256)          1048832     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "question_input (InputLayer)     (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 256)       0           image_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "question_embedding (Embedding)  (None, 26, 256)      3226112     question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "image_question_embedding (Conca (None, 27, 256)      0           reshape[0][0]                    \n",
      "                                                                 question_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "question_generator (LSTM)       [(None, 27, 512), (N 1574912     image_question_embedding[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 27, 12602)    6464826     question_generator[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "answer_dense_1 (Dense)          (None, 1000)         513000      question_generator[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "ignore_last_word (Lambda)       (None, 26, 12602)    0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "answer_classifier (Dense)       (None, 1000)         1001000     answer_dense_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 13,828,682\n",
      "Trainable params: 13,828,682\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharth/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 42s 52ms/step - loss: 11.2635 - ignore_last_word_loss: 5.6033 - answer_classifier_loss: 5.6602 - ignore_last_word_acc: 0.7017 - answer_classifier_acc: 0.2000 - val_loss: 6.5215 - val_ignore_last_word_loss: 1.9054 - val_answer_classifier_loss: 4.6161 - val_ignore_last_word_acc: 0.7415 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 21s 27ms/step - loss: 6.1016 - ignore_last_word_loss: 1.7030 - answer_classifier_loss: 4.3986 - ignore_last_word_acc: 0.7339 - answer_classifier_acc: 0.2313 - val_loss: 6.2433 - val_ignore_last_word_loss: 1.5883 - val_answer_classifier_loss: 4.6550 - val_ignore_last_word_acc: 0.7423 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 21s 27ms/step - loss: 5.6145 - ignore_last_word_loss: 1.4059 - answer_classifier_loss: 4.2086 - ignore_last_word_acc: 0.7437 - answer_classifier_acc: 0.2238 - val_loss: 6.4171 - val_ignore_last_word_loss: 1.4410 - val_answer_classifier_loss: 4.9761 - val_ignore_last_word_acc: 0.7587 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 21s 27ms/step - loss: 5.5410 - ignore_last_word_loss: 1.3179 - answer_classifier_loss: 4.2231 - ignore_last_word_acc: 0.7573 - answer_classifier_acc: 0.2137 - val_loss: 6.0984 - val_ignore_last_word_loss: 1.3951 - val_answer_classifier_loss: 4.7032 - val_ignore_last_word_acc: 0.7708 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 21s 27ms/step - loss: 5.3796 - ignore_last_word_loss: 1.2524 - answer_classifier_loss: 4.1272 - ignore_last_word_acc: 0.7678 - answer_classifier_acc: 0.2387 - val_loss: 6.2111 - val_ignore_last_word_loss: 1.3379 - val_answer_classifier_loss: 4.8732 - val_ignore_last_word_acc: 0.7840 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 21s 27ms/step - loss: 5.3164 - ignore_last_word_loss: 1.1866 - answer_classifier_loss: 4.1298 - ignore_last_word_acc: 0.7830 - answer_classifier_acc: 0.2213 - val_loss: 6.2966 - val_ignore_last_word_loss: 1.2962 - val_answer_classifier_loss: 5.0004 - val_ignore_last_word_acc: 0.7923 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 5.3105 - ignore_last_word_loss: 1.1428 - answer_classifier_loss: 4.1676 - ignore_last_word_acc: 0.7922 - answer_classifier_acc: 0.2038 - val_loss: 6.1997 - val_ignore_last_word_loss: 1.2702 - val_answer_classifier_loss: 4.9295 - val_ignore_last_word_acc: 0.7979 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 21s 27ms/step - loss: 5.2020 - ignore_last_word_loss: 1.1054 - answer_classifier_loss: 4.0966 - ignore_last_word_acc: 0.8015 - answer_classifier_acc: 0.2263 - val_loss: 6.1837 - val_ignore_last_word_loss: 1.2429 - val_answer_classifier_loss: 4.9408 - val_ignore_last_word_acc: 0.8008 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 21s 27ms/step - loss: 5.1420 - ignore_last_word_loss: 1.0593 - answer_classifier_loss: 4.0827 - ignore_last_word_acc: 0.8092 - answer_classifier_acc: 0.2375 - val_loss: 6.0675 - val_ignore_last_word_loss: 1.2113 - val_answer_classifier_loss: 4.8562 - val_ignore_last_word_acc: 0.8067 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 5.1309 - ignore_last_word_loss: 1.0140 - answer_classifier_loss: 4.1169 - ignore_last_word_acc: 0.8130 - answer_classifier_acc: 0.2337 - val_loss: 6.1221 - val_ignore_last_word_loss: 1.1906 - val_answer_classifier_loss: 4.9315 - val_ignore_last_word_acc: 0.8119 - val_answer_classifier_acc: 0.2000\n"
     ]
    }
   ],
   "source": [
    "ques = h5.File(\"../../data/data_train_val/data_prepro.h5\", \"r\")\n",
    "ques_train = ques['ques_train'][:n_train]\n",
    "ques_to_image_train = ques['img_pos_train'][:n_train] - 1\n",
    "\n",
    "\n",
    "ans_train = tf.keras.utils.to_categorical(y=ques['answers'][:n_train],\n",
    "                                          num_classes=n_answers)\n",
    "\n",
    "img_feat = h5.File(\"../../data/data_train_val/data_img.h5\", \"r\")\n",
    "img_train = np.array(img_feat['images_train'])[ques_to_image_train]\n",
    "\n",
    "model = VQANet(combine_type='show-and-tell', \n",
    "               question_embed_dim=question_embed_dim, \n",
    "               lstm_dim=lstm_dim, \n",
    "               n_answers=n_answers)\n",
    "\n",
    "print(model.model.summary())\n",
    "\n",
    "# Issue: ques_train input and output are the same.\n",
    "model.train(x_train=[img_train, ques_train], \n",
    "            y_train=[tf.keras.utils.to_categorical(y=ques_train, num_classes=VOCAB_SIZE), ans_train], \n",
    "            x_val=[], \n",
    "            y_val=[], \n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
