{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import h5py as h5\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12602\n",
    "MAX_QUESTION_LEN = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def qa_loss(y_true, y_pred):\n",
    "#     print(y_true[0])\n",
    "#     print(y_pred[0])\n",
    "#     q_true, a_true = y_true[0], y_true[1]\n",
    "#     q_pred, a_pred = y_pred[0][:-1], y_pred[1]\n",
    "    \n",
    "#     q_loss = K.sum(q_true * K.log(q_pred), axis=-1)\n",
    "#     q_loss = K.sum(q_loss, axis=-1)\n",
    "#     a_loss = K.sum(a_true * K.log(a_pred), axis=-1)\n",
    "#     loss = K.sum(y_true * K.log(y_pred), axis=-1)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQANet:\n",
    "    def __init__(self, combine_type, question_embed_dim, lstm_dim, n_answers):\n",
    "        self.combine_type = combine_type\n",
    "        self.question_embed_dim = question_embed_dim\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.n_answers = n_answers\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        if self.combine_type == 'show-and-tell':\n",
    "            image_features = tf.keras.layers.Input(shape=(4096,), \n",
    "                                                   dtype='float32')\n",
    "            \n",
    "            image_embedding = tf.keras.layers.Dense(units=self.question_embed_dim, \n",
    "                                                    activation='elu',\n",
    "                                                    name='image_embedding')(inputs=image_features)\n",
    "\n",
    "            image_embedding = tf.keras.layers.Reshape((1, self.question_embed_dim))(image_embedding)\n",
    "            \n",
    "            question_input = tf.keras.layers.Input(shape=(MAX_QUESTION_LEN,), \n",
    "                                                   dtype='int32',\n",
    "                                                   name='question_input')\n",
    "            \n",
    "            question_embedding = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, \n",
    "                                                           output_dim=self.question_embed_dim, \n",
    "                                                           input_length=MAX_QUESTION_LEN,\n",
    "                                                           name='question_embedding')(inputs=question_input)\n",
    "            \n",
    "            image_question_embedding = tf.keras.layers.Concatenate(axis=1, \n",
    "                                                                   name='image_question_embedding')(inputs=[image_embedding, question_embedding])\n",
    "            \n",
    "            question_features, last_h, _ = tf.keras.layers.LSTM(units=self.lstm_dim, \n",
    "                                                                return_sequences=True, \n",
    "                                                                return_state=True, \n",
    "                                                                name='question_generator')(inputs=image_question_embedding)\n",
    "\n",
    "            question_pred = tf.keras.layers.TimeDistributed(layer=tf.keras.layers.Dense(units=VOCAB_SIZE, \n",
    "                                                                  activation='softmax', \n",
    "                                                                  name='word_classifier'))(inputs=question_features)\n",
    "            \n",
    "            # question_pred[:-1] ignores the last output. Need to add <START> and <END>.\n",
    "            question_pred = tf.keras.layers.Lambda(lambda x: x[:, :-1, :], \n",
    "                                                   name='ignore_last_word')(inputs=question_pred)\n",
    "            \n",
    "            answer_fc1 = tf.keras.layers.Dense(units=1000,\n",
    "                                                activation='elu',\n",
    "                                                name='answer_dense_1')(inputs=last_h)\n",
    "            \n",
    "            answer_fc2 = tf.keras.layers.Dense(units=1000,\n",
    "                                                activation='elu',\n",
    "                                                name='answer_dense_2')(inputs=answer_fc1)\n",
    "            \n",
    "            answer_pred = tf.keras.layers.Dense(units=self.n_answers,\n",
    "                                                activation='softmax',\n",
    "                                                name='answer_classifier')(inputs=answer_fc2)\n",
    "            \n",
    "            self.model = tf.keras.Model(inputs=[image_features, question_input], \n",
    "                                        outputs=[answer_pred])  \n",
    "            \n",
    "            self.model.compile(loss='categorical_crossentropy', \n",
    "                               optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    def train(self, x_train, y_train, x_val, y_val, batch_size, epochs):\n",
    "        self.model.fit(x=x_train, \n",
    "                       y=y_train, \n",
    "                       batch_size=batch_size, \n",
    "                       epochs=epochs, \n",
    "                       verbose=1,\n",
    "                       validation_split=0.2,\n",
    "                       shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "question_embed_dim = 256\n",
    "lstm_dim = 512\n",
    "n_answers = 1000\n",
    "n_train = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_embedding (Dense)         (None, 256)          1048832     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "question_input (InputLayer)     (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 256)       0           image_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "question_embedding (Embedding)  (None, 26, 256)      3226112     question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "image_question_embedding (Conca (None, 27, 256)      0           reshape_1[0][0]                  \n",
      "                                                                 question_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "question_generator (LSTM)       [(None, 27, 512), (N 1574912     image_question_embedding[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "answer_dense_1 (Dense)          (None, 1000)         513000      question_generator[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 27, 12602)    6464826     question_generator[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "answer_dense_2 (Dense)          (None, 1000)         1001000     answer_dense_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "ignore_last_word (Lambda)       (None, 26, 12602)    0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "answer_classifier (Dense)       (None, 1000)         1001000     answer_dense_2[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 14,829,682\n",
      "Trainable params: 14,829,682\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharth/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 43s 53ms/step - loss: 12.3193 - ignore_last_word_loss: 6.6690 - answer_classifier_loss: 5.6503 - ignore_last_word_acc: 0.7142 - answer_classifier_acc: 0.1825 - val_loss: 6.6649 - val_ignore_last_word_loss: 1.7082 - val_answer_classifier_loss: 4.9567 - val_ignore_last_word_acc: 0.7463 - val_answer_classifier_acc: 0.0400\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 6.0537 - ignore_last_word_loss: 1.5854 - answer_classifier_loss: 4.4683 - ignore_last_word_acc: 0.7424 - answer_classifier_acc: 0.1913 - val_loss: 6.1687 - val_ignore_last_word_loss: 1.5068 - val_answer_classifier_loss: 4.6619 - val_ignore_last_word_acc: 0.7519 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 5.5594 - ignore_last_word_loss: 1.3546 - answer_classifier_loss: 4.2047 - ignore_last_word_acc: 0.7532 - answer_classifier_acc: 0.2325 - val_loss: 6.4100 - val_ignore_last_word_loss: 1.4275 - val_answer_classifier_loss: 4.9825 - val_ignore_last_word_acc: 0.7663 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 5.4871 - ignore_last_word_loss: 1.2993 - answer_classifier_loss: 4.1878 - ignore_last_word_acc: 0.7605 - answer_classifier_acc: 0.2188 - val_loss: 6.4415 - val_ignore_last_word_loss: 1.4148 - val_answer_classifier_loss: 5.0267 - val_ignore_last_word_acc: 0.7652 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 5.4259 - ignore_last_word_loss: 1.2591 - answer_classifier_loss: 4.1668 - ignore_last_word_acc: 0.7673 - answer_classifier_acc: 0.2287 - val_loss: 6.3197 - val_ignore_last_word_loss: 1.3796 - val_answer_classifier_loss: 4.9401 - val_ignore_last_word_acc: 0.7775 - val_answer_classifier_acc: 0.2000\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 5.3871 - ignore_last_word_loss: 1.2125 - answer_classifier_loss: 4.1746 - ignore_last_word_acc: 0.7772 - answer_classifier_acc: 0.2313 - val_loss: 6.2987 - val_ignore_last_word_loss: 1.3474 - val_answer_classifier_loss: 4.9513 - val_ignore_last_word_acc: 0.7794 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 5.3717 - ignore_last_word_loss: 1.1668 - answer_classifier_loss: 4.2049 - ignore_last_word_acc: 0.7861 - answer_classifier_acc: 0.2425 - val_loss: 6.2710 - val_ignore_last_word_loss: 1.3176 - val_answer_classifier_loss: 4.9534 - val_ignore_last_word_acc: 0.7927 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 22s 28ms/step - loss: 5.2867 - ignore_last_word_loss: 1.1339 - answer_classifier_loss: 4.1529 - ignore_last_word_acc: 0.7928 - answer_classifier_acc: 0.2200 - val_loss: 6.0582 - val_ignore_last_word_loss: 1.3049 - val_answer_classifier_loss: 4.7533 - val_ignore_last_word_acc: 0.7958 - val_answer_classifier_acc: 0.2000\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 5.2346 - ignore_last_word_loss: 1.1071 - answer_classifier_loss: 4.1275 - ignore_last_word_acc: 0.8006 - answer_classifier_acc: 0.2188 - val_loss: 6.2750 - val_ignore_last_word_loss: 1.2874 - val_answer_classifier_loss: 4.9875 - val_ignore_last_word_acc: 0.8031 - val_answer_classifier_acc: 0.2550\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 22s 27ms/step - loss: 5.2029 - ignore_last_word_loss: 1.0819 - answer_classifier_loss: 4.1211 - ignore_last_word_acc: 0.8065 - answer_classifier_acc: 0.2438 - val_loss: 6.2917 - val_ignore_last_word_loss: 1.2700 - val_answer_classifier_loss: 5.0217 - val_ignore_last_word_acc: 0.8065 - val_answer_classifier_acc: 0.2550\n"
     ]
    }
   ],
   "source": [
    "ques = h5.File(\"../../data/data_train_val/data_prepro.h5\", \"r\")\n",
    "ques_train = ques['ques_train'][:n_train]\n",
    "ques_to_image_train = ques['img_pos_train'][:n_train] - 1\n",
    "\n",
    "ans_train = tf.keras.utils.to_categorical(y=ques['answers'][:n_train],\n",
    "                                          num_classes=n_answers)\n",
    "\n",
    "img_feat = h5.File(\"../../data/data_train_val/data_img.h5\", \"r\")\n",
    "img_train = np.array(img_feat['images_train'])[ques_to_image_train]\n",
    "\n",
    "print(img_train.shape)\n",
    "\n",
    "model = VQANet(combine_type='show-and-tell', \n",
    "               question_embed_dim=question_embed_dim, \n",
    "               lstm_dim=lstm_dim, \n",
    "               n_answers=n_answers)\n",
    "\n",
    "print(model.model.summary())\n",
    "\n",
    "# Issue: ques_train input and output are the same.\n",
    "model.train(x_train=[img_train, ques_train], \n",
    "            y_train=[num_classes=VOCAB_SIZE), ans_train], \n",
    "            x_val=[], \n",
    "            y_val=[], \n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
